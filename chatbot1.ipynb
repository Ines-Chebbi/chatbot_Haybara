{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPNoKiMWr40FvO8u0CeX6Xk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"obUpumji7C1K","executionInfo":{"status":"ok","timestamp":1723458838430,"user_tz":-120,"elapsed":340,"user":{"displayName":"Chebbi ines","userId":"18240609720109891280"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# Beginning of the AI\n","class ChatBot():\n","    def __init__(self, name):\n","        print(\"----- starting up\", name, \"-----\")\n","        self.name = name\n","# Execute the AI\n","if __name__ == \"__main__\":\n","    ai = ChatBot(name=\"Ines\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QVdleuio7DDy","executionInfo":{"status":"ok","timestamp":1723458840218,"user_tz":-120,"elapsed":888,"user":{"displayName":"Chebbi ines","userId":"18240609720109891280"}},"outputId":"c2f62c2a-6cb6-40a0-dfb8-bdfea2a9b7fa"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["----- starting up Ines -----\n"]}]},{"cell_type":"code","source":["# Simple Rule-Based Chatbot in Python\n","#def chatbot_response(user_input):\n"," #   if \"hello\" in user_input.lower():\n","  #      return \"Hi there! How can I help you today?\"\n"," #   elif \"price\" in user_input.lower():\n","  #      return \"Our products range from $10 to $500.\"\n","   # else:\n","    #    return \"I'm sorry, I didn't understand that.\"\n","\n","#user_input = input(\"You: \")\n","#print(\"Bot:\", chatbot_response(user_input))\n"],"metadata":{"id":"tx_qgcUD8CQ2","executionInfo":{"status":"ok","timestamp":1723458840219,"user_tz":-120,"elapsed":10,"user":{"displayName":"Chebbi ines","userId":"18240609720109891280"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import nltk\n","import json\n","from nltk.corpus import wordnet\n","from nltk.stem import WordNetLemmatizer\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import random\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"jQzs3HMZFAei","executionInfo":{"status":"ok","timestamp":1723458852637,"user_tz":-120,"elapsed":12427,"user":{"displayName":"Chebbi ines","userId":"18240609720109891280"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import json\n","\n","# Assume 'data.json' is the file you uploaded\n","#with open('/content/intents1.json', 'r') as file:\n"," #   intents = json.load(file)\n","\n","# Access data\n","#print(intents)\n"],"metadata":{"id":"bWNy5UId9QXe","executionInfo":{"status":"ok","timestamp":1723458852638,"user_tz":-120,"elapsed":12,"user":{"displayName":"Chebbi ines","userId":"18240609720109891280"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["intents = {\n","  \"intents\": [\n","    {\n","      \"tag\": \"greeting\",\n","      \"patterns\": [\"Hi\", \"Hello\", \"Hey\", \"Good day\", \"How are you?\"],\n","      \"responses\": [\"Hello!\", \"Good to see you!\", \"Hi there, how can I help?\"],\n","    },\n","    {\n","      \"tag\": \"farewell\",\n","      \"patterns\": [\"Goodbye\", \"Bye\", \"See you later\", \"Talk to you later\"],\n","      \"responses\": [\"Sad to see you go :(\", \"Goodbye!\", \"Come back soon!\"],\n","\n","    },\n","    {\n","      \"tag\": \"creator\",\n","      \"patterns\": [\"Who created you?\", \"Who is your developer?\", \"Who made you?\"],\n","      \"responses\": [\"I was created by Chebbi Ines.\"],\n","\n","    },\n","    {\n","      \"tag\": \"identity\",\n","      \"patterns\": [\"What is your name?\", \"What should I call you?\", \"Who are you?\",\"What are you\",\"Introduce Yourself\"],\n","      \"responses\": [\"You can call me Haybara. I'm a Chatbot.\"],\n","\n","    },\n","    {\n","      \"tag\": \"vacation\",\n","      \"patterns\": [\"I want to spend the vacation in tunisia,what can you recommand?\", \"What are the must-visit places in Tunisia?\"],\n","      \"responses\": [\"Tunisia offers a rich mix of historical sites and natural beauty. Be sure to visit the ancient ruins of Carthage, explore the stunning blue-and-white village of Sidi Bou Said, and wander through the Medina of Tunis, a UNESCO World Heritage site. For a unique experience, head to the Sahara Desert for a camel trek or visit the well-preserved Roman amphitheater in El Djem.\"],\n","\n","    },\n","    {\n","      \"tag\": \"casual_greeting\",\n","      \"patterns\": [\"What's up?\", \"How are you?\", \"How you doing?\"],\n","       \"responses\": [\"I'm here to assist you with any questions or information you need. How can I assist you today?\"]\n","\n","     },\n","    {\n","      \"tag\": \"good_morning\",\n","      \"patterns\": [\"Good morning\", \"Morning\"],\n","      \"responses\": [\"Good morning! How can I assist you today?\"]\n","\n","     },\n","     {\n","       \"tag\": \"good_afternoon\",\n","       \"patterns\": [\"Good afternoon\", \"Afternoon\"],\n","        \"responses\": [\"Good afternoon! How can I assist you today?\"]\n","\n","      },\n","      {\n","      \"tag\": \"good_evening\",\n","      \"patterns\": [\"Good evening\", \"Evening\"],\n","       \"responses\": [\"Good evening! How can I assist you today?\"]\n","\n","         },\n","      {\n","        \"tag\": \"thank_you\",\n","        \"patterns\": [\"Thank you\", \"Thanks\"],\n","        \"responses\": [\"You're welcome! If you have any more questions, feel free to ask.\"]\n","\n","        },\n","       {\n","       \"tag\": \"sorry\",\n","      \"patterns\": [\"Sorry\", \"Apologies\"],\n","       \"responses\": [\"No problem! If there's anything else you need assistance with, feel free to let me know.\"]\n","\n","    },\n","    {\n","      \"tag\": \"time\",\n","      \"patterns\": [\"What's the best time of year to visit Tunisia?\"],\n","      \"responses\": [\"The best time to visit Tunisia is in the spring (March to May) or fall (September to November) when the weather is pleasant and temperatures are moderate\"]\n","\n","    },\n","    {\n","      \"tag\": \"dishes\",\n","      \"patterns\": [\"Can you recommend any traditional Tunisian dishes to try?\"],\n","      \"responses\": [\"Tunisian cuisine is a delightful fusion of Mediterranean and North African flavors. Be sure to try couscous, which is the national dish, typically served with lamb, chicken, or fish. Other must-try dishes include brik, a crispy pastry filled with egg and tuna, and lablabi, a comforting chickpea soup. Don't forget to taste harissa, a spicy chili paste that's a staple in Tunisian cooking.\"]\n","},\n","    {\n","      \"tag\": \"cultural experiences\",\n","      \"patterns\": [\"What are some cultural experiences in Tunisia\"],\n","      \"responses\": [\"In Tunisia, you can immerse yourself in local culture by visiting a traditional souk in the Medina, where you can buy handmade crafts and spices. Experience the vibrant atmosphere of a Tunisian café, sipping mint tea or strong coffee. Attending a local festival, like the Carthage Film Festival or the International Festival of the Sahara in Douz, will give you a taste of the country's rich cultural heritage.\"]\n","\n","    },\n","    {\n","      \"tag\": \"historical sites\",\n","      \"patterns\": [\"What historical sites can I explore in Tunisia?\"],\n","      \"responses\": [\"Tunisia is rich in historical sites. Key locations include Carthage: The ancient ruins of this once-powerful city include the Roman baths, amphitheater, and the Punic Ports. El Djem: Famous for its colossal Roman amphitheater, which is remarkably well-preserved and one of the largest in Africa. Kairouan: Known for its great mosque, one of the oldest and most important Islamic sites in the region. Dougga: An ancient Roman city with impressive ruins, including temples, a theater, and a triumphal arch.\"]\n","\n","    },\n","\n","    {\n","      \"tag\": \"beaches\",\n","      \"patterns\": [\"What are the best beaches to visit in Tunisia?\"],\n","      \"responses\": [\"Tunisia boasts beautiful Mediterranean beaches. Some of the best include:Hammamet: Known for its fine sand and clear waters, Hammamet is a popular beach destination with many resorts. Sousse: This city offers a mix of beautiful beaches and vibrant nightlife. The beaches are well-maintained and ideal for sunbathing and swimming. Djerba: This island features some of Tunisia’s most picturesque beaches, with golden sands and calm, shallow waters. Zarzis: Located in the southern part of the country, Zarzis offers a more tranquil beach experience with stunning, unspoiled coastline.\"]\n","\n","    }\n","\n","\n","]\n","}"],"metadata":{"id":"e5t0_4OOMMzF","executionInfo":{"status":"ok","timestamp":1723458852638,"user_tz":-120,"elapsed":10,"user":{"displayName":"Chebbi ines","userId":"18240609720109891280"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import nltk # Import the nltk library\n","\n","nltk.download('punkt')\n","nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a9T4UhnAFBYG","executionInfo":{"status":"ok","timestamp":1723458853596,"user_tz":-120,"elapsed":967,"user":{"displayName":"Chebbi ines","userId":"18240609720109891280"}},"outputId":"50b3423a-f934-4450-971c-fc5671935c47"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["stemmer = WordNetLemmatizer()\n","\n","words = []\n","classes = []\n","documents = []\n","ignore_words = ['?']\n","\n","\n","for intent in intents['intents']:\n","    for pattern in intent['patterns']:\n","        # Tokenize each word in the sentence\n","        w = nltk.word_tokenize(pattern)\n","        # Add to our words list\n","        words.extend(w)\n","        # Add to documents in our corpus\n","        documents.append((w, intent['tag']))\n","        # Add to our classes list\n","        if intent['tag'] not in classes:\n","            classes.append(intent['tag'])\n","\n","\n","words = [stemmer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n","words = sorted(list(set(words)))\n","\n","\n","classes = sorted(list(set(classes)))\n","\n","print(len(documents), \"documents\")\n","print(len(classes), \"classes\", classes)\n","print(len(words), \"unique lemmatized words\", words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hyb-mrlWFLA-","executionInfo":{"status":"ok","timestamp":1723458859538,"user_tz":-120,"elapsed":5948,"user":{"displayName":"Chebbi ines","userId":"18240609720109891280"}},"outputId":"b72ac2e7-b6ec-487c-ec1a-53d08afecb94"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["37 documents\n","16 classes ['beaches', 'casual_greeting', 'creator', 'cultural experiences', 'dishes', 'farewell', 'good_afternoon', 'good_evening', 'good_morning', 'greeting', 'historical sites', 'identity', 'sorry', 'thank_you', 'time', 'vacation']\n","66 unique lemmatized words [\"'s\", ',', 'afternoon', 'any', 'apology', 'are', 'beach', 'best', 'bye', 'call', 'can', 'created', 'cultural', 'day', 'developer', 'dish', 'doing', 'evening', 'experience', 'explore', 'good', 'goodbye', 'hello', 'hey', 'hi', 'historical', 'how', 'i', 'in', 'introduce', 'is', 'later', 'made', 'morning', 'must-visit', 'name', 'of', 'place', 'recommand', 'recommend', 'see', 'should', 'site', 'some', 'sorry', 'spend', 'talk', 'thank', 'thanks', 'the', 'time', 'to', 'traditional', 'try', 'tunisia', 'tunisian', 'up', 'vacation', 'visit', 'want', 'what', 'who', 'year', 'you', 'your', 'yourself']\n"]}]},{"cell_type":"code","source":["training = []\n","output = []\n","output_empty = [0] * len(classes)\n","\n","# Training set, bag of words for each sentence\n","for doc in documents:\n","    # Initialize our bag of words\n","    bag = []\n","    # List of tokenized words for the pattern\n","    pattern_words = doc[0]\n","    # Lemmatize each word\n","    pattern_words = [stemmer.lemmatize(word.lower()) for word in pattern_words]\n","    # Create our bag of words array\n","    for w in words:\n","        bag.append(1) if w in pattern_words else bag.append(0)\n","\n","    # Output is a '0' for each tag and '1' for the current tag\n","    output_row = list(output_empty)\n","    output_row[classes.index(doc[1])] = 1\n","\n","    training.append([bag, output_row])\n","\n","random.shuffle(training)\n","def synonym_replacement(tokens, limit):\n","    augmented_sentences = []\n","    for i in range(len(tokens)):\n","        synonyms = []\n","        for syn in wordnet.synsets(tokens[i]):\n","            for lemma in syn.lemmas():\n","                synonyms.append(lemma.name())\n","        if len(synonyms) > 0:\n","            num_augmentations = min(limit, len(synonyms))\n","            sampled_synonyms = random.sample(synonyms, num_augmentations)\n","            for synonym in sampled_synonyms:\n","                augmented_tokens = tokens[:i] + [synonym] + tokens[i + 1:]\n","                augmented_sentences.append(' '.join(augmented_tokens))\n","    return augmented_sentences\n","\n","\n","# Augment the training data using synonym replacement\n","augmented_data = []\n","limit_per_tag = 100\n","for i, doc in enumerate(training):\n","    bag, output_row = doc\n","    tokens = [words[j] for j in range(len(words)) if bag[j] == 1]\n","    augmented_sentences = synonym_replacement(tokens, limit_per_tag)\n","    for augmented_sentence in augmented_sentences:\n","        augmented_bag = [1 if augmented_sentence.find(word) >= 0 else 0 for word in words]\n","        augmented_data.append([augmented_bag, output_row])\n","\n","training = np.array(training, dtype=object)\n","augmented_data_array = np.array(augmented_data, dtype=object)\n","\n","combined_data = np.concatenate((training, augmented_data_array), axis=0)\n","random.shuffle(combined_data)"],"metadata":{"id":"GerbJnl3FLDW","executionInfo":{"status":"ok","timestamp":1723458860038,"user_tz":-120,"elapsed":507,"user":{"displayName":"Chebbi ines","userId":"18240609720109891280"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","\n","def separate_data_by_tags(data):\n","    data_by_tags = {}\n","    for d in data:\n","        tag = tuple(d[1])\n","        if tag not in data_by_tags:\n","            data_by_tags[tag] = []\n","        data_by_tags[tag].append(d)\n","    return data_by_tags.values()\n","\n","\n","separated_data = separate_data_by_tags(combined_data)\n","\n","# Lists to store training and testing data\n","training_data = []\n","testing_data = []\n","# Split each tag's data into training and testing sets\n","for tag_data in separated_data:\n","    train_data, test_data = train_test_split(tag_data, test_size=0.2, random_state=42)\n","    training_data.extend(train_data)\n","    testing_data.extend(test_data)\n","\n","\n","random.shuffle(training_data)\n","random.shuffle(testing_data)\n","\n","# Convert training and testing data back to np.array\n","train_x = np.array([d[0] for d in training_data])\n","train_y = np.array([d[1] for d in training_data])\n","test_x = np.array([d[0] for d in testing_data])\n","test_y = np.array([d[1] for d in testing_data])"],"metadata":{"id":"EjZPdozJFLF_","executionInfo":{"status":"ok","timestamp":1723458860403,"user_tz":-120,"elapsed":376,"user":{"displayName":"Chebbi ines","userId":"18240609720109891280"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","\n","class NeuralNetwork(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(NeuralNetwork, self).__init__()\n","        self.fc1 = nn.Linear(input_size, hidden_size)\n","        self.relu1 = nn.ReLU()\n","        self.bn1 = nn.BatchNorm1d(hidden_size)\n","        self.dropout1 = nn.Dropout(0.2)\n","\n","        self.fc2 = nn.Linear(hidden_size, hidden_size)\n","        self.relu2 = nn.ReLU()\n","        self.bn2 = nn.BatchNorm1d(hidden_size)\n","        self.dropout2 = nn.Dropout(0.2)\n","\n","        self.fc3 = nn.Linear(hidden_size, output_size)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.relu1(x)\n","        x = self.bn1(x)\n","        x = self.dropout1(x)\n","\n","        x = self.fc2(x)\n","        x = self.relu2(x)\n","        x = self.bn2(x)\n","        x = self.dropout2(x)\n","\n","        x = self.fc3(x)\n","        output = self.softmax(x)\n","        return output\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, x, y):\n","        self.x = x\n","        self.y = y\n","\n","    def __len__(self):\n","        return len(self.x)\n","\n","    def __getitem__(self, idx):\n","        return self.x[idx], self.y[idx]\n","\n","def accuracy(predictions, targets):\n","    predicted_labels = torch.argmax(predictions, dim=1)\n","    true_labels = torch.argmax(targets, dim=1)\n","    correct = (predicted_labels == true_labels).sum().item()\n","    total = targets.size(0)\n","    return correct / total\n","\n","def test_model(model, test_loader, criterion):\n","    model.eval()\n","    total_loss = 0.0\n","    total_accuracy = 0.0\n","    num_batches = len(test_loader)\n","\n","    with torch.no_grad():\n","        for inputs, targets in test_loader:\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","            total_loss += loss.item() * inputs.size(0)\n","            total_accuracy += accuracy(outputs, targets) * inputs.size(0)\n","\n","    average_loss = total_loss / len(test_loader.dataset)\n","    average_accuracy = total_accuracy / len(test_loader.dataset)\n","    return average_loss, average_accuracy\n"],"metadata":{"id":"G04_ONX_FLJj","executionInfo":{"status":"ok","timestamp":1723458860403,"user_tz":-120,"elapsed":11,"user":{"displayName":"Chebbi ines","userId":"18240609720109891280"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Create DataLoader for training and testing data\n","train_x = torch.tensor(train_x).float()\n","train_y = torch.tensor(train_y).float()\n","test_x = torch.tensor(test_x).float()\n","test_y = torch.tensor(test_y).float()\n","\n","batch_size = 64\n","train_dataset = CustomDataset(train_x, train_y)\n","test_dataset = CustomDataset(test_x, test_y)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Define the model, loss function, and optimizer\n","input_size = len(train_x[0])\n","hidden_size = 8\n","output_size = len(train_y[0])\n","model = NeuralNetwork(input_size, hidden_size, output_size)\n","criterion = nn.BCELoss()\n","optimizer = optim.Adam(model.parameters())\n","# Train the model and evaluate on the testing set\n","num_epochs = 50\n","for epoch in range(num_epochs):\n","    # Training\n","    model.train()\n","    running_loss = 0.0\n","    running_acc = 0.0\n","    for inputs, targets in train_loader:\n","        # Forward pass\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Update statistics\n","        running_loss += loss.item() * inputs.size(0)\n","        running_acc += accuracy(outputs, targets) * inputs.size(0)\n","\n","    # Calculate average training loss and accuracy for the epoch\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","    epoch_acc = running_acc / len(train_loader.dataset)\n","\n","    # Print training loss and accuracy for each epoch\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_acc:.4f}\")\n","\n","    # Evaluate on the testing set\n","    test_loss, test_accuracy = test_model(model, test_loader, criterion)\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Testing Loss: {test_loss:.4f}, Testing Accuracy: {test_accuracy:.4f}\")\n","\n","# Save the trained model\n","torch.save(model.state_dict(), 'model.pth')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lBMI1LUOGUuc","executionInfo":{"status":"ok","timestamp":1723458869803,"user_tz":-120,"elapsed":9407,"user":{"displayName":"Chebbi ines","userId":"18240609720109891280"}},"outputId":"4460a87d-136f-4111-a819-0b77acda0d3a"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/50], Training Loss: 0.2274, Training Accuracy: 0.1063\n","Epoch [1/50], Testing Loss: 0.2171, Testing Accuracy: 0.2043\n","Epoch [2/50], Training Loss: 0.2046, Training Accuracy: 0.2404\n","Epoch [2/50], Testing Loss: 0.1944, Testing Accuracy: 0.4471\n","Epoch [3/50], Training Loss: 0.1869, Training Accuracy: 0.3916\n","Epoch [3/50], Testing Loss: 0.1753, Testing Accuracy: 0.5312\n","Epoch [4/50], Training Loss: 0.1745, Training Accuracy: 0.4991\n","Epoch [4/50], Testing Loss: 0.1584, Testing Accuracy: 0.7428\n","Epoch [5/50], Training Loss: 0.1625, Training Accuracy: 0.5604\n","Epoch [5/50], Testing Loss: 0.1459, Testing Accuracy: 0.7692\n","Epoch [6/50], Training Loss: 0.1517, Training Accuracy: 0.6060\n","Epoch [6/50], Testing Loss: 0.1323, Testing Accuracy: 0.7909\n","Epoch [7/50], Training Loss: 0.1417, Training Accuracy: 0.6193\n","Epoch [7/50], Testing Loss: 0.1215, Testing Accuracy: 0.8173\n","Epoch [8/50], Training Loss: 0.1321, Training Accuracy: 0.6521\n","Epoch [8/50], Testing Loss: 0.1165, Testing Accuracy: 0.8245\n","Epoch [9/50], Training Loss: 0.1273, Training Accuracy: 0.6794\n","Epoch [9/50], Testing Loss: 0.1051, Testing Accuracy: 0.8317\n","Epoch [10/50], Training Loss: 0.1203, Training Accuracy: 0.6849\n","Epoch [10/50], Testing Loss: 0.0991, Testing Accuracy: 0.8293\n","Epoch [11/50], Training Loss: 0.1142, Training Accuracy: 0.6988\n","Epoch [11/50], Testing Loss: 0.0896, Testing Accuracy: 0.8510\n","Epoch [12/50], Training Loss: 0.1073, Training Accuracy: 0.6970\n","Epoch [12/50], Testing Loss: 0.0845, Testing Accuracy: 0.8462\n","Epoch [13/50], Training Loss: 0.1040, Training Accuracy: 0.7110\n","Epoch [13/50], Testing Loss: 0.0785, Testing Accuracy: 0.8630\n","Epoch [14/50], Training Loss: 0.0991, Training Accuracy: 0.7195\n","Epoch [14/50], Testing Loss: 0.0735, Testing Accuracy: 0.8678\n","Epoch [15/50], Training Loss: 0.0943, Training Accuracy: 0.7262\n","Epoch [15/50], Testing Loss: 0.0705, Testing Accuracy: 0.8726\n","Epoch [16/50], Training Loss: 0.0912, Training Accuracy: 0.7341\n","Epoch [16/50], Testing Loss: 0.0652, Testing Accuracy: 0.8774\n","Epoch [17/50], Training Loss: 0.0901, Training Accuracy: 0.7280\n","Epoch [17/50], Testing Loss: 0.0613, Testing Accuracy: 0.8822\n","Epoch [18/50], Training Loss: 0.0869, Training Accuracy: 0.7328\n","Epoch [18/50], Testing Loss: 0.0591, Testing Accuracy: 0.8822\n","Epoch [19/50], Training Loss: 0.0855, Training Accuracy: 0.7341\n","Epoch [19/50], Testing Loss: 0.0558, Testing Accuracy: 0.8774\n","Epoch [20/50], Training Loss: 0.0815, Training Accuracy: 0.7438\n","Epoch [20/50], Testing Loss: 0.0528, Testing Accuracy: 0.8918\n","Epoch [21/50], Training Loss: 0.0799, Training Accuracy: 0.7577\n","Epoch [21/50], Testing Loss: 0.0507, Testing Accuracy: 0.8942\n","Epoch [22/50], Training Loss: 0.0741, Training Accuracy: 0.7772\n","Epoch [22/50], Testing Loss: 0.0483, Testing Accuracy: 0.9014\n","Epoch [23/50], Training Loss: 0.0727, Training Accuracy: 0.7820\n","Epoch [23/50], Testing Loss: 0.0461, Testing Accuracy: 0.9135\n","Epoch [24/50], Training Loss: 0.0744, Training Accuracy: 0.7808\n","Epoch [24/50], Testing Loss: 0.0437, Testing Accuracy: 0.9135\n","Epoch [25/50], Training Loss: 0.0714, Training Accuracy: 0.7832\n","Epoch [25/50], Testing Loss: 0.0422, Testing Accuracy: 0.9135\n","Epoch [26/50], Training Loss: 0.0694, Training Accuracy: 0.7881\n","Epoch [26/50], Testing Loss: 0.0404, Testing Accuracy: 0.9135\n","Epoch [27/50], Training Loss: 0.0672, Training Accuracy: 0.7845\n","Epoch [27/50], Testing Loss: 0.0390, Testing Accuracy: 0.9135\n","Epoch [28/50], Training Loss: 0.0658, Training Accuracy: 0.7881\n","Epoch [28/50], Testing Loss: 0.0378, Testing Accuracy: 0.9135\n","Epoch [29/50], Training Loss: 0.0653, Training Accuracy: 0.7923\n","Epoch [29/50], Testing Loss: 0.0366, Testing Accuracy: 0.9135\n","Epoch [30/50], Training Loss: 0.0664, Training Accuracy: 0.7887\n","Epoch [30/50], Testing Loss: 0.0354, Testing Accuracy: 0.9135\n","Epoch [31/50], Training Loss: 0.0631, Training Accuracy: 0.7966\n","Epoch [31/50], Testing Loss: 0.0344, Testing Accuracy: 0.9135\n","Epoch [32/50], Training Loss: 0.0608, Training Accuracy: 0.8075\n","Epoch [32/50], Testing Loss: 0.0330, Testing Accuracy: 0.9135\n","Epoch [33/50], Training Loss: 0.0611, Training Accuracy: 0.7960\n","Epoch [33/50], Testing Loss: 0.0322, Testing Accuracy: 0.9135\n","Epoch [34/50], Training Loss: 0.0604, Training Accuracy: 0.8009\n","Epoch [34/50], Testing Loss: 0.0312, Testing Accuracy: 0.9135\n","Epoch [35/50], Training Loss: 0.0601, Training Accuracy: 0.8027\n","Epoch [35/50], Testing Loss: 0.0310, Testing Accuracy: 0.9135\n","Epoch [36/50], Training Loss: 0.0587, Training Accuracy: 0.8075\n","Epoch [36/50], Testing Loss: 0.0298, Testing Accuracy: 0.9135\n","Epoch [37/50], Training Loss: 0.0587, Training Accuracy: 0.8057\n","Epoch [37/50], Testing Loss: 0.0296, Testing Accuracy: 0.9135\n","Epoch [38/50], Training Loss: 0.0592, Training Accuracy: 0.8136\n","Epoch [38/50], Testing Loss: 0.0290, Testing Accuracy: 0.9135\n","Epoch [39/50], Training Loss: 0.0582, Training Accuracy: 0.8069\n","Epoch [39/50], Testing Loss: 0.0278, Testing Accuracy: 0.9135\n","Epoch [40/50], Training Loss: 0.0563, Training Accuracy: 0.8142\n","Epoch [40/50], Testing Loss: 0.0277, Testing Accuracy: 0.9135\n","Epoch [41/50], Training Loss: 0.0566, Training Accuracy: 0.8063\n","Epoch [41/50], Testing Loss: 0.0274, Testing Accuracy: 0.9231\n","Epoch [42/50], Training Loss: 0.0533, Training Accuracy: 0.8257\n","Epoch [42/50], Testing Loss: 0.0273, Testing Accuracy: 0.9231\n","Epoch [43/50], Training Loss: 0.0525, Training Accuracy: 0.8264\n","Epoch [43/50], Testing Loss: 0.0267, Testing Accuracy: 0.9231\n","Epoch [44/50], Training Loss: 0.0535, Training Accuracy: 0.8227\n","Epoch [44/50], Testing Loss: 0.0263, Testing Accuracy: 0.9303\n","Epoch [45/50], Training Loss: 0.0549, Training Accuracy: 0.8179\n","Epoch [45/50], Testing Loss: 0.0255, Testing Accuracy: 0.9231\n","Epoch [46/50], Training Loss: 0.0533, Training Accuracy: 0.8227\n","Epoch [46/50], Testing Loss: 0.0255, Testing Accuracy: 0.9231\n","Epoch [47/50], Training Loss: 0.0544, Training Accuracy: 0.8179\n","Epoch [47/50], Testing Loss: 0.0252, Testing Accuracy: 0.9255\n","Epoch [48/50], Training Loss: 0.0519, Training Accuracy: 0.8264\n","Epoch [48/50], Testing Loss: 0.0248, Testing Accuracy: 0.9231\n","Epoch [49/50], Training Loss: 0.0516, Training Accuracy: 0.8288\n","Epoch [49/50], Testing Loss: 0.0248, Testing Accuracy: 0.9231\n","Epoch [50/50], Training Loss: 0.0520, Training Accuracy: 0.8185\n","Epoch [50/50], Testing Loss: 0.0241, Testing Accuracy: 0.9231\n"]}]},{"cell_type":"code","source":["input_size = len(train_x[0])\n","hidden_size = 8\n","output_size = len(train_y[0])\n","print(input_size, hidden_size, output_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kNK2wCVR-UnN","executionInfo":{"status":"ok","timestamp":1723458869804,"user_tz":-120,"elapsed":81,"user":{"displayName":"Chebbi ines","userId":"18240609720109891280"}},"outputId":"4ce755fb-637b-4bf3-bae6-7e9b1e867872"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["66 8 16\n"]}]},{"cell_type":"code","source":["def load_model(model_path, input_size, hidden_size, output_size):\n","    model = NeuralNetwork(input_size, hidden_size, output_size)\n","    model.load_state_dict(torch.load(model_path))\n","    model.eval()\n","    return model\n","\n","# Function to preprocess the input sentence\n","def preprocess_sentence(sentence, words):\n","    sentence_words = sentence.lower().split()\n","    sentence_words = [word for word in sentence_words if word in words]\n","    return sentence_words\n","\n","# Function to convert the preprocessed sentence into a feature vector\n","def sentence_to_features(sentence_words, words):\n","    features = [1 if word in sentence_words else 0 for word in words]\n","    return torch.tensor(features).float().unsqueeze(0)\n","\n","# Function to generate a response using the trained model\n","def generate_response(sentence, model, words, classes):\n","    sentence_words = preprocess_sentence(sentence, words)\n","    if len(sentence_words) == 0:\n","        return \"I'm sorry, but I don't understand. Can you please rephrase or provide more information?\"\n","    features = sentence_to_features(sentence_words, words)\n","    with torch.no_grad():\n","        outputs = model(features)\n","\n","    probabilities, predicted_class = torch.max(outputs, dim=1)\n","    confidence = probabilities.item()\n","    predicted_tag = classes[predicted_class.item()]\n","\n","\n","    for intent in intents['intents']:\n","            if intent['tag'] == predicted_tag:\n","                return random.choice(intent['responses'])\n","\n","    return \"I'm sorry, but I'm not sure how to respond to that.\""],"metadata":{"id":"lXY_YbTfGUxP","executionInfo":{"status":"ok","timestamp":1723458869804,"user_tz":-120,"elapsed":69,"user":{"displayName":"Chebbi ines","userId":"18240609720109891280"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["len(train_y[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9gacjDsuz6X4","executionInfo":{"status":"ok","timestamp":1723458869804,"user_tz":-120,"elapsed":68,"user":{"displayName":"Chebbi ines","userId":"18240609720109891280"}},"outputId":"654de2b5-1da7-4241-f30c-e1582cdf0cef"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["16"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["model_path = 'model.pth'\n","input_size = len(words)\n","hidden_size = 8\n","output_size = len(classes)\n","model = load_model(model_path, input_size, hidden_size, output_size)\n","\n","# Test the chatbot response\n","print('Hello! I am HIBARA. How can I help you today? Type \"quit\" to exit.')\n","while True:\n","    user_input = input('> ')\n","    if user_input.lower() == 'quit':\n","        break\n","    response = generate_response(user_input, model, words, classes)\n","    print(response)\n",""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":512},"id":"9c06182lGU0g","executionInfo":{"status":"error","timestamp":1723458963512,"user_tz":-120,"elapsed":93773,"user":{"displayName":"Chebbi ines","userId":"18240609720109891280"}},"outputId":"3219505d-d67b-4ae3-a84b-230da398f8cd"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello! I am HIBARA. How can I help you today? Type \"quit\" to exit.\n","> hi\n","Good to see you!\n","> me too, can u tell me about tunisia\n","Good morning! How can I assist you today?\n","> i wanna spend the vacation in tunisia\n","Good morning! How can I assist you today?\n","> i wanna spend the vacation in tunisia, what places can i go to\n","Good morning! How can I assist you today?\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"Interrupted by user","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-1561276eac4e>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Hello! I am HIBARA. How can I help you today? Type \"quit\" to exit.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'> '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}]},{"cell_type":"code","source":["import json\n","import pickle\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import torch\n","from torch import nn, optim\n","import random\n","\n","# Example training data\n","data =  [\n","    \"Hi\", \"Hello\", \"Hey\", \"Good day\", \"How are you?\",\n","    \"Goodbye\", \"Bye\", \"See you later\", \"Talk to you later\",\n","    \"Who created you?\", \"Who is your developer?\", \"Who made you?\",\n","    \"What is your name?\", \"What should I call you?\", \"Who are you?\", \"What are you?\", \"Introduce Yourself\",\n","    \"I want to spend the vacation in Tunisia, what can you recommend?\",\n","    \"What are the must-visit places in Tunisia?\",\n","    \"What's up?\", \"How are you?\", \"How you doing?\",\n","    \"Good morning\", \"Morning\", \"Good afternoon\", \"Afternoon\", \"Good evening\", \"Evening\",\n","    \"Thank you\", \"Thanks\", \"Sorry\", \"Apologies\",\n","    \"What's the best time of year to visit Tunisia?\",\n","    \"Can you recommend any traditional Tunisian dishes to try?\",\n","    \"What are some cultural experiences in Tunisia?\",\n","    \"What historical sites can I explore in Tunisia?\",\n","    \"What are the best beaches to visit in Tunisia?\"\n","]\n","\n","\n","# Create and fit the vectorizer\n","vectorizer = TfidfVectorizer()\n","# Extract the actual sentences from the training data structure\n","training_sentences_text = [sentence for sentence in data] # Assuming 'data' contains the actual sentences\n","X_train = vectorizer.fit_transform(training_sentences_text) # Pass a list of strings\n","\n"],"metadata":{"id":"fcr2eeJILsfk","executionInfo":{"status":"ok","timestamp":1723458968666,"user_tz":-120,"elapsed":355,"user":{"displayName":"Chebbi ines","userId":"18240609720109891280"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Make sure model and vectorizer are loaded and fitted\n","if not hasattr(vectorizer, 'vocabulary_'):\n","    print(\"Fitting the vectorizer...\")  # Debugging line\n","    vectorizer.fit(intents)  # Replace with actual training data\n","    print(\"Vectorizer fitted.\")  # Debugging line\n","\n","if model is None:\n","    print(\"Loading model...\")  # Debugging line\n","    model = load_model('model.pth')  # Ensure this function works as expected\n","    print(\"Model loaded.\")  # Debugging line\n"],"metadata":{"id":"5L-LEKxD3QDB","executionInfo":{"status":"ok","timestamp":1723458973477,"user_tz":-120,"elapsed":395,"user":{"displayName":"Chebbi ines","userId":"18240609720109891280"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["input_size = len(train_x[0])\n","hidden_size = 8\n","output_size = len(train_y[0])\n","print(input_size, hidden_size, output_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LPop1LUn-mfO","executionInfo":{"status":"ok","timestamp":1723458975690,"user_tz":-120,"elapsed":367,"user":{"displayName":"Chebbi ines","userId":"18240609720109891280"}},"outputId":"340fc5e2-3332-433c-8c39-961750efa9e2"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["66 8 16\n"]}]},{"cell_type":"code","source":["# Save the fitted vectorizer\n","#with open('vectorizer.pkl', 'wb') as f:\n","#    pickle.dump(vectorizer, f)\n","\n","#print(\"Vectorizer fitted and saved.\")"],"metadata":{"id":"2BOif0k21qt2","executionInfo":{"status":"aborted","timestamp":1723458963515,"user_tz":-120,"elapsed":10,"user":{"displayName":"Chebbi ines","userId":"18240609720109891280"}}},"execution_count":null,"outputs":[]}]}